{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Azure Supercomputer User Guide","text":""},{"location":"#for-aiml-infrastructure-teams-operating-large-scale-gpu-clusters","title":"For AI/ML infrastructure teams operating large-scale GPU clusters","text":"<p>This guide helps infrastructure engineers, ops teams, and ML researchers deploy and operate GPU supercomputing environments on Azure. It focuses on NDv4 and NDv5 VM SKUs and provides practical guidance for setup, validation, benchmarking, and performance optimization.</p> <p>Whether you're bringing up your first NDv5 cluster or tuning NCCL for a 1,024-GPU run, this guide aims to make Azure's high-performance infrastructure accessible and operationally reliable.</p> <p>Topics covered include:</p> <ul> <li>Subscription and quota prep</li> <li>Deployment architectures and automation</li> <li>VM SKU selection and hardware topologies</li> <li>Node validation and health checks</li> <li>Performance benchmarking with NCCL</li> <li>Guest Health Reporting (GHR)</li> <li>InfiniBand topology tuning</li> <li>Telemetry and observability</li> <li>Diagnostic tools and troubleshooting workflows</li> </ul> <p>Contributions welcome. Reach out to your Microsoft team or open a PR if this site is hosted on GitHub.</p>"},{"location":"appendices/","title":"Appendices","text":"<p>This section contains reference material, scripts, and diagnostic guidance to support users operating GPU supercomputing clusters on Azure.</p>"},{"location":"appendices/#a-diagnostic-scripts","title":"A. Diagnostic Scripts","text":""},{"location":"appendices/#node-health-check-azhpc","title":"Node Health Check (AzHPC)","text":"<p>The AzHPC validation toolkit includes a modular node health check script:</p> <pre><code>git clone https://github.com/Azure/azhpc-validation\ncd azhpc-validation\nbash scripts/run-validation.sh\n</code></pre> <p>Includes checks for:</p> <ul> <li>GPU enumeration and driver status</li> <li>ECC errors</li> <li>PCIe/NVLink/IB connectivity</li> <li>NCCL functionality</li> <li>Clock/thermal status</li> </ul>"},{"location":"appendices/#nccl-benchmark-scripts","title":"NCCL Benchmark Scripts","text":"<p>Preconfigured NCCL benchmark wrappers can be found in the same repo or customized:</p> <pre><code>mpirun -np 8 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH \\\n  ./build/all_reduce_perf -b 8 -e 1G -f 2 -g 1\n</code></pre>"},{"location":"appendices/#b-common-issues-and-signatures","title":"B. Common Issues and Signatures","text":"Symptom Possible Cause Tool Missing GPU GPU failure, driver issue <code>nvidia-smi</code>, NHC Low NCCL bandwidth SHARP off, job not packed <code>all_reduce_perf</code>, ToRset InfiniBand link down Cable/NIC/switch issue <code>ibstat</code>, <code>perfquery</code> ECC error spike Faulty GPU <code>nvidia-smi -q</code>, DCGM PCIe bus errors NUMA misalignment, system misconfig <code>lspci</code>, <code>dmesg</code>"},{"location":"appendices/#c-reference-links","title":"C. Reference Links","text":"<ul> <li>AzHPC GitHub</li> <li>Moneo GitHub</li> <li>GHR API Docs</li> <li>NVIDIA NCCL</li> </ul>"},{"location":"appendices/#d-feedback-contributions","title":"D. Feedback &amp; Contributions","text":"<p>This guide is open to customer feedback. If you notice outdated info or would like to contribute improvements, reach out to your Microsoft account team or submit a pull request if hosted on GitHub.</p> <p>End of Guide.</p>"},{"location":"benchmarking/","title":"Benchmarking","text":"<p>This section describes how to benchmark your Azure supercomputing cluster to verify expected performance and identify potential bottlenecks. Benchmarks also serve as a pre-check for production readiness and support engagement.</p>"},{"location":"benchmarking/#1-why-benchmark","title":"1. Why Benchmark?","text":"<ul> <li>Validate cluster configuration (e.g., topology, SHARP enablement)</li> <li>Establish performance baselines for future regressions</li> <li>Identify underperforming nodes or links</li> <li>Support escalation by demonstrating hardware-level anomalies</li> </ul>"},{"location":"benchmarking/#2-nccl-benchmarks","title":"2. NCCL Benchmarks","text":"<p>NCCL is the standard collective communication library for multi-GPU workloads using NVLink and InfiniBand.</p> <p>Clone and build the tests:</p> <pre><code>git clone https://github.com/NVIDIA/nccl-tests.git\ncd nccl-tests\nmake MPI=1\n</code></pre> <p>Then run:</p> <pre><code>mpirun -np 8 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH \\\n  ./build/all_reduce_perf -b 8 -e 1G -f 2 -g 1\n</code></pre> <p>Use the number of GPUs you have available, and ensure each rank maps to a separate GPU.</p>"},{"location":"benchmarking/#3-sharp-vs-non-sharp-output","title":"3. SHARP vs Non-SHARP Output","text":"Test Pattern SHARP-enabled (NDv4) Non-SHARP AllReduce 1GB ~180 GB/s ~120 GB/s AllReduce 256MB ~90\u2013120 GB/s ~60\u201380 GB/s <p>Performance depends on node locality and job packing. Use ToRset information to diagnose.</p>"},{"location":"benchmarking/#4-interpreting-results","title":"4. Interpreting Results","text":"<ul> <li>Flat or low throughput across sizes suggests topology misalignment or SHARP not engaged</li> <li>One GPU consistently slower can indicate a bad PCIe lane or thermal throttling</li> <li>High variability between runs = likely job placement issue</li> </ul> <p>Plot and compare runs to a known-good benchmark from your team or Microsoft.</p>"},{"location":"benchmarking/#5-additional-tests","title":"5. Additional Tests","text":"<ul> <li><code>ib_read_bw</code> / <code>ib_write_bw</code> \u2013 raw IB throughput per link</li> <li><code>dcgmi dmon -e 1000</code> \u2013 GPU perf counters</li> <li><code>nvidia-smi nvlink --status</code> \u2013 validate NVLink health</li> </ul> <p>Next: Telemetry &amp; Observability</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>This section describes how to deploy Azure GPU Supercomputing infrastructure, with options for CLI-based provisioning, infrastructure-as-code tools, and key networking considerations.</p>"},{"location":"deployment/#1-choose-a-deployment-method","title":"1. Choose a Deployment Method","text":"<p>Azure Supercomputing clusters can be deployed using the following options:</p> <ul> <li>Azure CLI \u2013 for lightweight manual provisioning and testing  </li> <li>Bicep or ARM templates \u2013 for reproducible and auditable deployments  </li> <li>Terraform \u2013 popular among infrastructure teams for cloud-agnostic deployment  </li> <li>AzHPC \u2013 Microsoft-supported toolkit for deploying tightly coupled HPC clusters with InfiniBand  </li> </ul> <p>Recommendation: Use AzHPC for complex topologies or when IB tuning is required.</p>"},{"location":"deployment/#2-define-your-topology","title":"2. Define Your Topology","text":"<p>Define:</p> <ul> <li>Desired VM SKU (NDv4 or NDv5)  </li> <li>Number of nodes  </li> <li>InfiniBand network topology (e.g., flat, SHARP-enabled, non-SHARP)  </li> <li>Placement policies (e.g., proximity placement groups)  </li> </ul> <p>Use the appropriate parameters or variable files depending on your tooling.</p>"},{"location":"deployment/#3-configure-networking","title":"3. Configure Networking","text":"<p>Ensure the following:</p> <ul> <li>VNet and subnet are provisioned with sufficient IPs  </li> <li>Accelerated networking is enabled  </li> <li>NSGs allow SSH, telemetry, and any required workload ports  </li> <li>If using IB, ensure the correct partitioning and ToR topology alignment  </li> </ul>"},{"location":"deployment/#4-provision-resources","title":"4. Provision Resources","text":"<p>Example CLI steps:</p> <p>```bash az group create -n myResourceGroup -l eastus</p> <p>az vm create \\   --resource-group myResourceGroup \\   --name myVM \\   --image OpenLogic:CentOS-HPC:7_9:latest \\   --size Standard_ND96asr_v4 \\   --vnet-name myVNet \\   --subnet mySubnet \\   --admin-username azureuser \\   --ssh-key-values ~/.ssh/id_rsa.pub   ``` Replace with your VM SKU, region, and networking details.</p>"},{"location":"deployment/#5-post-deployment-validation","title":"5. Post-Deployment Validation","text":"<p>After deployment, verify:</p> <ul> <li>Node health (see the Validation section)  </li> <li>IB topology and functionality (see the Topology section)  </li> <li>Telemetry pipeline is functional (see the Telemetry section)  </li> </ul>"},{"location":"deployment/#6-automation-and-scaling","title":"6. Automation and Scaling","text":"<p>We recommend integrating deployment pipelines into your CI/CD system for reproducibility. For scale-out, consider:</p> <ul> <li>VM Scale Sets (VMSS) with custom image  </li> <li>Azure CycleCloud  </li> <li>AzHPC scripts with looped host creation  </li> </ul> <p>Next: VM SKU Reference</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section provides the essential setup steps to prepare for using Azure's AI Supercomputing infrastructure. It covers subscription readiness, quota configuration, access roles, and optional onboarding for advanced features like Guest Health Reporting (GHR).</p>"},{"location":"getting-started/#1-subscription-preparation","title":"1. Subscription Preparation","text":"<p>Before deploying GPU clusters, ensure the following:</p> <ul> <li>You have access to an Azure subscription in the correct region.</li> <li>Sufficient quota for the target VM SKU (NDv4 or NDv5) is available.</li> <li>Required resource providers are registered:<ul> <li>Microsoft.Network</li> </ul> </li> </ul> <p>Use the Azure CLI to validate and request quota increases if needed.</p>"},{"location":"getting-started/#2-role-assignments-and-access-control","title":"2. Role Assignments and Access Control","text":"<p>Assign the following roles to the appropriate identities in your Azure subscription:</p> <ul> <li>Contributor or Owner: to deploy infrastructure and manage resources.</li> <li>Impact Reporter: for GHR operations (if enabled).</li> <li>Reader: for monitoring and telemetry dashboards.</li> </ul> <p>Ensure your automation identities (e.g., Terraform, Bicep, AzHPC) have adequate permissions.</p>"},{"location":"getting-started/#3-register-for-guest-health-reporting-optional","title":"3. Register for Guest Health Reporting (Optional)","text":"<p>Guest Health Reporting (GHR) enables qualified customers to notify Azure about faulty hardware nodes.</p> <p>To register:</p> <ol> <li>Go to your Azure subscription.</li> <li>In Resource Providers, register <code>Microsoft.Impact</code>.</li> <li>In Preview Features, register <code>Allow Impact Reporting</code>.</li> <li>Under Access Control (IAM), assign the Impact Reporter role to the app or user that will report issues.</li> <li>Fill out the Onboarding Questionnaire.</li> </ol> <p>See the Guest Health Reporting section for usage details.</p>"},{"location":"getting-started/#4-next-steps","title":"4. Next Steps","text":"<p>Once your subscription is ready and roles assigned, proceed to the Deployment Guide to launch your supercomputing cluster.</p>"},{"location":"ghr/","title":"Guest Health Reporting (GHR)","text":"<p>Guest Health Reporting (GHR) is a mechanism that allows customers to notify Azure about suspected hardware issues with specific nodes. It is available to approved customers operating supported VM SKUs like NDv4 and NDv5.</p>"},{"location":"ghr/#1-what-is-ghr","title":"1. What is GHR?","text":"<p>GHR enables external users to flag potentially faulty virtual machines to Microsoft. These reports contribute to Azure's hardware telemetry and support processes, accelerating detection and remediation of underlying issues.</p>"},{"location":"ghr/#2-who-can-use-ghr","title":"2. Who Can Use GHR?","text":"<p>GHR is currently in preview and is only available to approved customers. To request access:</p> <ul> <li>Register the <code>Microsoft.Impact</code> resource provider</li> <li>Enable the preview feature <code>Allow Impact Reporting</code></li> <li>Assign the <code>Impact Reporter</code> role to your reporting identity</li> <li>Complete the onboarding form (link in Getting Started)</li> </ul>"},{"location":"ghr/#3-how-ghr-works","title":"3. How GHR Works","text":"<p>Once enabled:</p> <ol> <li>You detect a node with a suspected fault (via validation, logs, repeated failures, etc.)</li> <li>Your system (or you) sends a signed POST request to the GHR API with impact details</li> <li>Azure logs and triages the report; correlated reports trigger deeper diagnostics or node removal</li> </ol> <p>Reports are not immediate triggers\u2014they are signals in a broader telemetry system.</p>"},{"location":"ghr/#4-reporting-an-impact","title":"4. Reporting an Impact","text":"<p>To report an impact, POST to the following endpoint:</p> <pre><code>https://impact.api.azure.com/impact/v1/report\n</code></pre> <p>With a body like:</p> <pre><code>{\n  \"subscriptionId\": \"&lt;your-subscription-id&gt;\",\n  \"resourceUri\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Compute/virtualMachines/&lt;vm-name&gt;\",\n  \"impactedComponents\": [\n    {\n      \"impactCategory\": \"GPU\",\n      \"impactType\": \"DegradedPerformance\",\n      \"timestamp\": \"2024-04-10T22:30:00Z\"\n    }\n  ]\n}\n</code></pre> <p>Make sure your identity has the <code>Impact Reporter</code> role and your app is registered in Azure AD.</p>"},{"location":"ghr/#5-supported-impact-categories","title":"5. Supported Impact Categories","text":"Category Type Example GPU DegradedPerformance ECC errors, frequent resets IB Unreachable Node fails NCCL or link tests CPU UnexpectedReboot Node crashes during workload PCIe BandwidthThrottle PCIe/NVLink bottleneck observed"},{"location":"ghr/#6-best-practices","title":"6. Best Practices","text":"<ul> <li>Only report when confident the issue is hardware-related</li> <li>Include timestamps and context if possible</li> <li>Integrate into automated diagnostic pipelines for scale</li> </ul> <p>Next: InfiniBand Topology</p>"},{"location":"operations/","title":"Operations","text":"<p>This section covers best practices for day-to-day operations of Azure GPU supercomputing clusters, including workload monitoring, failure remediation, and using Guest Health Reporting (GHR).</p>"},{"location":"operations/#1-monitoring-jobs-and-node-health","title":"1. Monitoring Jobs and Node Health","text":"<p>For job-level and cluster-level visibility:</p> <ul> <li>Use Prometheus and Grafana for GPU/CPU/memory metrics</li> <li>Monitor GPU utilization, thermal state, ECC errors, and memory usage via <code>nvidia-smi</code> or DCGM</li> <li>InfiniBand traffic and errors can be tracked using <code>perfquery</code>, <code>ibdiagnet</code>, or <code>infiniband-exporter</code></li> <li>Use AzHPC telemetry or Moneo if supported in your cluster</li> </ul>"},{"location":"operations/#2-common-failure-modes","title":"2. Common Failure Modes","text":"<p>Watch for:</p> <ul> <li>Node hangs or unresponsiveness</li> <li>Repeated job failures on specific nodes</li> <li>ECC or PCIe errors</li> <li>GPUs missing from <code>nvidia-smi</code></li> <li>InfiniBand degradation or disconnections</li> </ul> <p>Many of these are detected during pre-job NHC or post-failure diagnostics.</p>"},{"location":"operations/#3-failure-remediation","title":"3. Failure Remediation","text":"<p>Steps:</p> <ol> <li>Drain the node from your scheduler (e.g., <code>scontrol update nodename=XXX state=drain reason=\"validation fail\"</code>).</li> <li>Run AzHPC NHC or custom diagnostics scripts.</li> <li>Compare results with historical telemetry.</li> <li>If issue persists and GHR is enabled, report it.</li> </ol> <p>Document steps and timestamps to correlate with Azure support logs if escalation is required.</p>"},{"location":"operations/#4-node-reallocation","title":"4. Node Reallocation","text":"<p>If you observe flaky behavior (intermittent failures), consider:</p> <ul> <li>Manually deallocating and reallocating the node</li> <li>Reimaging the node with your base image</li> <li>Cross-validating in different jobs or under stress testing</li> </ul> <p>Avoid building long-term automation around reallocation\u2014it\u2019s a workaround, not a fix.</p>"},{"location":"operations/#5-guest-health-reporting-ghr","title":"5. Guest Health Reporting (GHR)","text":"<p>For supported customers, GHR enables impact reporting and tracking hardware incidents.</p> <ul> <li>Register using the onboarding steps in Getting Started</li> <li>For full usage, see GHR</li> </ul> <p>GHR can be integrated with job failure detection systems to auto-report problematic nodes.</p> <p>Next: Guest Health Reporting (GHR)</p>"},{"location":"telemetry/","title":"Telemetry &amp; Observability","text":"<p>This section describes how to monitor the health and performance of your Azure supercomputing cluster using telemetry tools. Proper observability helps detect anomalies, prevent silent failures, and ensure peak utilization.</p>"},{"location":"telemetry/#1-what-to-monitor","title":"1. What to Monitor","text":"<p>Key system-level and job-level metrics include:</p> <ul> <li>GPU metrics: utilization, memory usage, ECC errors, temperature, throttling</li> <li>CPU and memory usage: saturation and NUMA behavior</li> <li>InfiniBand: throughput, link failures, retransmits</li> <li>Node state: availability, reboots, hangs</li> <li>Scheduler state: queue delays, idle GPUs, job eviction rates</li> </ul>"},{"location":"telemetry/#2-tools","title":"2. Tools","text":"<p>You can use a combination of Azure-native, open-source, and AzHPC-provided tools.</p>"},{"location":"telemetry/#moneo","title":"Moneo","text":"<p>Moneo is an Azure-native observability stack tailored for GPU clusters. It includes:</p> <ul> <li>Node exporter (Prometheus)</li> <li>NVIDIA DCGM exporter</li> <li>Infiniband exporter</li> <li>Pre-built Grafana dashboards</li> </ul> <p>To deploy:</p> <pre><code>git clone https://github.com/Azure/moneo\ncd moneo\n./install.sh\n</code></pre> <p>It will deploy telemetry agents and set up a monitoring pipeline with Prometheus and Grafana.</p>"},{"location":"telemetry/#prometheus-grafana-custom","title":"Prometheus + Grafana (Custom)","text":"<p>If you have an existing Prometheus setup, integrate exporters such as:</p> <ul> <li><code>node_exporter</code></li> <li><code>dcgm-exporter</code></li> <li><code>infiniband-exporter</code></li> <li><code>slurm-exporter</code> (if using Slurm)</li> </ul> <p>Use Grafana dashboards to correlate resource usage with job timing and errors.</p>"},{"location":"telemetry/#3-azure-monitoring-optional","title":"3. Azure Monitoring (Optional)","text":"<p>You can also integrate with Azure Monitor or Log Analytics:</p> <ul> <li>Send custom logs and metrics using <code>telegraf</code></li> <li>Use Azure Monitor Workbooks for dashboards</li> <li>Create alerts on hardware errors, GPU underutilization, or unexpected node reboots</li> </ul>"},{"location":"telemetry/#4-best-practices","title":"4. Best Practices","text":"<ul> <li>Tag all nodes by role (headnode, compute) for filtering</li> <li>Correlate telemetry with Slurm/Kubernetes logs</li> <li>Set up alerts for high ECC error rates or job starvation</li> <li>Archive telemetry from known-good clusters as performance baselines</li> </ul> <p>Next: Appendices</p>"},{"location":"topology/","title":"InfiniBand Topology","text":"<p>This section describes how InfiniBand (IB) networks are structured in Azure ND-series clusters, and how users can discover, interpret, and optimize their IB topology for maximum performance.</p>"},{"location":"topology/#1-infiniband-in-azure-supercomputing","title":"1. InfiniBand in Azure Supercomputing","text":"<p>Each VM is connected via HDR or NDR InfiniBand NICs, typically one NIC per GPU pair:</p> <ul> <li>NDv4: 4\u00d7 200 Gbps HDR  </li> <li>NDv5: 4\u00d7 400 Gbps NDR  </li> </ul> <p>The physical layout includes multiple layers of switches. Your topology may support SHARP (Scalable Hierarchical Aggregation and Reduction Protocol), which accelerates collective operations in NCCL.</p>"},{"location":"topology/#2-sharp-vs-non-sharp-topology","title":"2. SHARP vs Non-SHARP Topology","text":"Topology Type Description SHARP-enabled Uses special switch hierarchy to offload collective ops; faster NCCL all-reduce Non-SHARP Standard fat-tree or leaf-spine IB; performance relies more on node placement and job packing <p>SHARP is available only in select regions and clusters. Ask your support team for confirmation.</p>"},{"location":"topology/#3-discovering-topology","title":"3. Discovering Topology","text":"<p>Use these tools:</p> <ul> <li><code>ibstat</code> and <code>ibstatus</code> \u2013 check link state  </li> <li><code>ibdiagnet</code> \u2013 discover fabric topology  </li> <li><code>perfquery</code> \u2013 monitor port performance  </li> <li><code>infiniband-exporter</code> \u2013 Prometheus exporter for IB metrics  </li> </ul> <p>You may also inspect <code>/sys/class/infiniband/*/ports/*/rate</code> and use <code>mlxconfig</code> for NIC-level information.</p>"},{"location":"topology/#4-generating-torsets","title":"4. Generating ToRsets","text":"<p>ToRsets (Top-of-Rack groupings) help align job placement with physical locality for performance.</p> <p>To generate a ToRset from NCCL tests:</p> <pre><code>git clone https://github.com/Azure/azhpc-utils\ncd azhpc-utils/scripts\nbash create_torset.sh -d &lt;test-output-dir&gt;\n</code></pre> <p>Feed NCCL test logs to the script; it will output JSON with recommended packing sets.</p>"},{"location":"topology/#5-job-packing-best-practices","title":"5. Job Packing Best Practices","text":"<ul> <li>Use ToRset info to minimize cross-rack communication  </li> <li>Prefer filling whole racks before crossing to the next  </li> <li>When SHARP is enabled, try to keep all nodes in the same aggregation group  </li> </ul> <p>Next: Benchmarking</p>"},{"location":"validation/","title":"Validation &amp; Health Checks","text":"<p>This section covers how to verify that your Azure supercomputing cluster is functioning correctly after deployment. It includes hardware validation, integration checks, and best practices for ongoing health monitoring.</p>"},{"location":"validation/#1-cluster-bring-up-validation","title":"1. Cluster Bring-up Validation","text":"<p>After provisioning, validate that all nodes:</p> <ul> <li>Are reachable via SSH</li> <li>Have the correct VM SKU and expected hardware configuration</li> <li>Are configured with the expected InfiniBand topology</li> <li>Appear in your scheduler or orchestration system (e.g., Slurm, Kubernetes)</li> </ul> <p>Use <code>lshw</code>, <code>nvidia-smi</code>, and <code>ibstat</code> to confirm basic hardware presence and status.</p>"},{"location":"validation/#2-node-health-checks","title":"2. Node Health Checks","text":"<p>Azure provides a Node Health Check (NHC) toolkit via the AzHPC project, which verifies:</p> <ul> <li>GPU enumeration and driver status</li> <li>ECC error state</li> <li>InfiniBand connectivity and performance</li> <li>PCIe/NVMe health</li> <li>Clock and thermal sanity</li> <li>NCCL functional tests</li> </ul> <p>To run NHC:</p> <pre><code>git clone https://github.com/Azure/azhpc-validation\ncd azhpc-validation\nbash scripts/run-validation.sh\n</code></pre> <p>You can run this post-deployment and periodically as a diagnostic.</p>"},{"location":"validation/#3-scheduler-integration","title":"3. Scheduler Integration","text":"<p>If using Slurm:</p> <ul> <li>Confirm that nodes are visible with <code>sinfo</code></li> <li>Nodes should be marked <code>idle</code> or <code>alloc</code> once healthy</li> <li>Slurm NHC plugins can run pre-job checks and evict failing nodes</li> </ul> <p>If using Kubernetes:</p> <ul> <li>Confirm GPU node readiness with <code>kubectl get nodes -o wide</code></li> <li>Ensure <code>nvidia-device-plugin</code> is running</li> <li>Optionally, use a DaemonSet to run health checks regularly</li> </ul>"},{"location":"validation/#4-common-failures","title":"4. Common Failures","text":"<p>These issues should be remediated or reported via GHR:</p> <ul> <li>GPUs not visible in <code>nvidia-smi</code></li> <li>InfiniBand link down or degraded (<code>ibstat</code>, <code>ibstatus</code>)</li> <li>Persistent ECC or double-bit errors</li> <li>PCIe bus errors or NUMA misalignment</li> <li>Nodes that hang or reboot under load</li> </ul>"},{"location":"validation/#5-best-practices","title":"5. Best Practices","text":"<ul> <li>Run NHC after every deployment and weekly thereafter</li> <li>Log all validation output centrally</li> <li>Automate node drain + notify on health failure</li> <li>Track flaky vs consistently bad nodes separately</li> </ul> <p>Next: Operations</p>"},{"location":"vm-skus/","title":"VM SKU Reference","text":"<p>This section provides detailed specifications and usage guidance for Azure ND-series VM SKUs commonly used in supercomputing workloads.</p>"},{"location":"vm-skus/#nd-a100-v4-ndv4","title":"ND A100 v4 (NDv4)","text":"<p>Hardware Specs:</p> <ul> <li>8\u00d7 NVIDIA A100 80GB SXM GPUs (NVLink 600 GB/s GPU-GPU)</li> <li>Dual AMD EPYC 7V73X CPUs (96 cores total)</li> <li>900 GB/s memory bandwidth per GPU</li> <li>1.6 TB system RAM</li> <li>4\u00d7 200 Gbps HDR InfiniBand NICs (Mellanox HCAs)</li> <li>2\u00d7 1.9TB NVMe SSDs</li> </ul> <p>Topology:</p> <ul> <li>Full NVLink mesh (each GPU connected to every other GPU)</li> <li>GPUs connected to CPUs via PCIe 4.0 switches</li> <li>1 NIC per GPU pair (affects SHARP topology)</li> </ul> <p>Recommended Use:</p> <ul> <li>Large-scale AI model training</li> <li>NCCL-based multi-GPU workloads</li> <li>SHARP-enabled collective comms (if supported by IB fabric)</li> <li>Tight coupling with Slurm / MPI</li> </ul>"},{"location":"vm-skus/#nd-h100-v5-ndv5","title":"ND H100 v5 (NDv5)","text":"<p>Hardware Specs:</p> <ul> <li>8\u00d7 NVIDIA H100 80GB SXM GPUs (NVLink 900 GB/s GPU-GPU)</li> <li>Dual Intel Sapphire Rapids CPUs (112 cores total)</li> <li>1.8 TB system RAM</li> <li>4\u00d7 400 Gbps NDR InfiniBand NICs</li> <li>2\u00d7 NVMe local drives</li> </ul> <p>Topology:</p> <ul> <li>Full NVLink (NVSwitch)</li> <li>PCIe Gen5 root complex</li> <li>Each NIC is dedicated to a GPU pair (as in NDv4), but with 400 Gbps bandwidth</li> </ul> <p>Recommended Use:</p> <ul> <li>GPT-style LLM training</li> <li>Transformer-heavy models with high flops/param density</li> <li>Better perf/Watt vs NDv4</li> <li>Use where NVLink bandwidth or PCIe bottlenecks were a constraint on NDv4</li> </ul>"},{"location":"vm-skus/#sku-selection-guidance","title":"SKU Selection Guidance","text":"Workload Type Suggested SKU FP32 CNN Training NDv4 FP16/BF16 LLM Training NDv5 Multi-node NCCL (SHARP) NDv4 (if SHARP enabled) Large batch inference NDv5 Custom kernels or legacy CUDA apps NDv4 or NDv5 depending on dependency set <p>Note: SHARP collectives require SHARP-compatible IB topology. Confirm with your support team.</p>"},{"location":"vm-skus/#availability","title":"Availability","text":"<p>VM SKU availability may vary by Azure region. For production-scale deployments, confirm SKU capacity with your Azure account team.</p> <p>Next: Validation &amp; Health Checks</p>"}]}