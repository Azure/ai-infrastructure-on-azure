apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: {{ include "llm-training.fullname" . }}
  labels:
    {{- include "llm-training.labels" . | nindent 4 }}
{{- if .Values.kueue.queueName }}
    kueue.x-k8s.io/queue-name: {{ .Values.kueue.queueName }}
{{- end }}
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            {{- include "llm-training.selectorLabels" . | nindent 12 }}
            role: master
        spec:
          {{- with .Values.nodeSelector }}
          nodeSelector:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.affinity }}
          affinity:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.tolerations }}
          tolerations:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          containers:
          - name: pytorch
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
            - "bash"
            - "-c"
            args:
            - |
              env && \
              ulimit -s unlimited && \
              ulimit -l unlimited && \
              ulimit -a && \
              LOG_FILE="/tmp/$(hostname).out" && \
              composer \
                --world_size {{ mul .Values.training.nodes .Values.training.gpusPerNode }} \
                --node_rank $RANK \
                --master_addr $MASTER_ADDR \
                --master_port $MASTER_PORT \
                --nproc {{ .Values.training.gpusPerNode }} \
                --verbose \
                /llm-foundry/scripts/train/train.py \
                /llm-foundry/scripts/train/yamls/pretrain/{{ .Values.model.config }}.yaml \
                {{- range $key, $value := .Values.yamlUpdates }}
                {{ $key }}={{ $value }} \
                {{- end }}
              2>&1 | tee "$LOG_FILE"
              EXIT_CODE=$?
              {{- if .Values.logs.copyLogsDir }}
              mkdir -p {{ .Values.logs.copyLogsDir }}
              cp "$LOG_FILE" {{ .Values.logs.copyLogsDir }}/ || echo "Failed to copy logs"
              {{- end }}
              exit $EXIT_CODE
            env:
            {{- include "llm-training.nccl-env" . | nindent 12 }}
            {{- include "llm-training.sharp-env" . | nindent 12 }}
            resources:
              requests:
                {{ .Values.resources.gpuResource }}: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
              limits:
                {{ .Values.resources.gpuResource }}: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
            volumeMounts:
            - name: data-storage
              mountPath: {{ .Values.storage.mount }}
            - name: shm
              mountPath: /dev/shm
            securityContext:
              privileged: true
              capabilities:
                add:
                - IPC_LOCK
          volumes:
          - name: data-storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: {{ .Values.resources.shmSize }}
    {{- if gt (.Values.training.nodes | int) 1 }}
    Worker:
      replicas: {{ sub (.Values.training.nodes | int) 1 }}
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            {{- include "llm-training.selectorLabels" . | nindent 12 }}
            role: worker
        spec:
          {{- with .Values.nodeSelector }}
          nodeSelector:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.affinity }}
          affinity:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.tolerations }}
          tolerations:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          containers:
          - name: pytorch
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
            - "bash"
            - "-c"
            args:
            - |
              env && \
              ulimit -s unlimited && \
              ulimit -l unlimited && \
              ulimit -a && \
              LOG_FILE="/tmp/$(hostname).out" && \
              composer \
                --world_size {{ mul .Values.training.nodes .Values.training.gpusPerNode }} \
                --node_rank $RANK \
                --master_addr $MASTER_ADDR \
                --master_port $MASTER_PORT \
                --nproc {{ .Values.training.gpusPerNode }} \
                --verbose \
                /llm-foundry/scripts/train/train.py \
                /llm-foundry/scripts/train/yamls/pretrain/{{ .Values.model.config }}.yaml \
                {{- range $key, $value := .Values.yamlUpdates }}
                {{ $key }}={{ $value }} \
                {{- end }}
              2>&1 | tee "$LOG_FILE"
              EXIT_CODE=$?
              {{- if .Values.logs.copyLogsDir }}
              mkdir -p {{ .Values.logs.copyLogsDir }}
              cp "$LOG_FILE" {{ .Values.logs.copyLogsDir }}/ || echo "Failed to copy logs"
              {{- end }}
              exit $EXIT_CODE
            env:
            {{- include "llm-training.nccl-env" . | nindent 12 }}
            {{- include "llm-training.sharp-env" . | nindent 12 }}
            resources:
              requests:
                {{ .Values.resources.gpuResource }}: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
              limits:
                {{ .Values.resources.gpuResource }}: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
            volumeMounts:
            - name: data-storage
              mountPath: {{ .Values.storage.mount }}
            - name: shm
              mountPath: /dev/shm
            securityContext:
              privileged: true
              capabilities:
                add:
                - IPC_LOCK
          volumes:
          - name: data-storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: {{ .Values.resources.shmSize }}

    {{- end }}

