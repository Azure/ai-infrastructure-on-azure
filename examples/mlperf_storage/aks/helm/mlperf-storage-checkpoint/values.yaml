# Default values for mlperf-storage-checkpoint
image:
  repository: ghcr.io/azure/ai-infrastructure-on-azure/mlperf-storage
  tag: latest
  pullPolicy: Always

# Kueue integration (optional)
# Set queueName to enable Kueue workload management
kueue:
  queueName: "" # e.g., "gpu-local-queue" to use Kueue

mpi:
  workers: 2
  slotsPerWorker: 8
  cleanPodPolicy: Running
  backoffLimit: 0
launcher:
  resources:
    requests:
      cpu: "1"
      memory: "16Gi"
    limits:
      cpu: "1"
      memory: "16Gi"
worker:
  resources:
    requests:
      memory: "256Gi"
    limits:
      memory: "256Gi"
benchmark:
  numCheckpoints: 10
  model: llama3-70b
  clientHostMemoryGiB: 32
  numCheckpointsWrite: 10
  numCheckpointsRead: 10
storage:
  pvcName: "pvcName"
  mountPath: "/mnt/storage"
  checkpointDir: "checkpoints"
  workloadRoot: "mlperf-workload"
  resultsDir: "results"
nodeSelector: {}
tolerations: []
affinity: {}
