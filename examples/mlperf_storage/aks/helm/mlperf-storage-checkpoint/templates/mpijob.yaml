apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: {{ include "mlperf-storage.fullname" . }}
  labels:
    {{- include "mlperf-storage.labels" . | nindent 4 }}
spec:
  slotsPerWorker: {{ .Values.mpi.slotsPerWorker }}
  runPolicy:
    cleanPodPolicy: {{ .Values.mpi.cleanPodPolicy }}
    backoffLimit: {{ .Values.mpi.backoffLimit }}
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        metadata:
          labels:
            {{- include "mlperf-storage.selectorLabels" . | nindent 12 }}
            role: launcher
        spec:
          restartPolicy: OnFailure
          containers:
          - name: mpi-launcher
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
              - /bin/bash
              - -c
              - |
                set -euo pipefail
                HOSTS=$(sed 's/ slots=/:/' /etc/mpi/hostfile | paste -sd ',' -)
                START_TS=$(date +%s)
                TIMEOUT=300
                IFS=',' read -r -a HOST_ARR <<< "${HOSTS}"
                while true; do
                  ALL_READY=1
                  for entry in "${HOST_ARR[@]}"; do
                    host="${entry%%:*}"
                    if ! ssh -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=5 "${host}" true 2>/dev/null; then
                      ALL_READY=0
                    fi
                  done
                  if [ "${ALL_READY}" -eq 1 ]; then
                    break
                  fi
                  ELAPSED=$(( $(date +%s)-START_TS ))
                  if [ "${ELAPSED}" -ge "${TIMEOUT}" ]; then
                    exit 1
                  fi
                  sleep 5
                done
                sleep 5
                $(which mlpstorage) checkpointing run \
                  --allow-run-as-root \
                  --num-processes "{{ mul .Values.mpi.workers .Values.mpi.slotsPerWorker }}" \
                  --results-dir {{ .Values.storage.mountPath }}/{{ .Values.storage.resultsDir }} \
                  --checkpoint-folder {{ .Values.storage.mountPath }}/{{ .Values.storage.checkpointDir }} \
                  --model {{ .Values.benchmark.model }} \
                  --num-checkpoints-write "{{ .Values.benchmark.numCheckpointsWrite }}" \
                  --num-checkpoints-read "{{ .Values.benchmark.numCheckpointsRead }}" \
                  --client-host-memory-in-gb "{{ .Values.benchmark.clientHostMemoryGiB }}" \
                  --hosts "${HOSTS}"
            volumeMounts:
            - name: storage
              mountPath: {{ .Values.storage.mountPath }}
            resources:
              {{- toYaml .Values.launcher.resources | nindent 14 }}
          volumes:
          - name: storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
    Worker:
      replicas: {{ .Values.mpi.workers }}
      template:
        metadata:
          labels:
            {{- include "mlperf-storage.selectorLabels" . | nindent 12 }}
            role: worker
        spec:
          containers:
          - name: mpi-worker
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
              - /usr/sbin/sshd
              - -De
            volumeMounts:
            - name: storage
              mountPath: {{ .Values.storage.mountPath }}
            resources:
              requests:
                cpu: "{{ .Values.mpi.slotsPerWorker }}"
                memory: {{ .Values.worker.resources.requests.memory | quote }}
              limits:
                cpu: "{{ .Values.mpi.slotsPerWorker }}"
                memory: {{ .Values.worker.resources.limits.memory | quote }}
          volumes:
          - name: storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
