MLPerf Storage Checkpoint deployment succeeded.

OVERVIEW
	This release runs the MLPerf Storage checkpoint workload (write + read).

STATUS COMMANDS
	# MPIJob status
	kubectl get mpijob {{ include "mlperf-storage.fullname" . }} -n {{ .Release.Namespace }}

	# Pods (launcher + workers)
	kubectl get pods -l app.kubernetes.io/instance={{ .Release.Name }} -n {{ .Release.Namespace }}

	# Launcher logs (benchmark progress)
	kubectl logs -f -l app.kubernetes.io/name={{ include "mlperf-storage.name" . }},role=launcher -n {{ .Release.Namespace }}

	# Worker logs (usually quiet except sshd)
	kubectl logs -l app.kubernetes.io/name={{ include "mlperf-storage.name" . }},role=worker -n {{ .Release.Namespace }}

RESULTS & CHECKPOINTS
	Results directory:      {{ .Values.storage.mountPath }}/{{ .Values.storage.resultsDir }}
	Checkpoint directory:   {{ .Values.storage.mountPath }}/{{ .Values.storage.checkpointDir }}

	Copy results locally:
		LAUNCHER_POD=$(kubectl get pods -l app.kubernetes.io/name={{ include "mlperf-storage.name" . }},role=launcher -n {{ .Release.Namespace }} -o jsonpath='{.items[0].metadata.name}')
		kubectl cp ${LAUNCHER_POD}:{{ .Values.storage.mountPath }}/{{ .Values.storage.resultsDir }} ./results -n {{ .Release.Namespace }}

CONFIGURATION SUMMARY
	Release Name:            {{ .Release.Name }}
	Namespace:               {{ .Release.Namespace }}
	Workers:                 {{ .Values.mpi.workers }}
	Slots / Worker (CPU):    {{ .Values.mpi.slotsPerWorker }}
	Total MPI Processes:     {{ mul .Values.mpi.workers .Values.mpi.slotsPerWorker }}
	Model:                   {{ .Values.benchmark.model }}
	Checkpoints (write/read): {{ .Values.benchmark.numCheckpointsWrite }} / {{ .Values.benchmark.numCheckpointsRead }}
	Client Host Mem (GiB):   {{ .Values.benchmark.clientHostMemoryGiB }}
	PVC Name:                {{ .Values.storage.pvcName }}
	Image:                   {{ .Values.image.repository }}:{{ .Values.image.tag }}

BENCHMARK INVOCATION (Template)
	mlpstorage checkpointing run \
		--allow-run-as-root \
		--num-processes {{ mul .Values.mpi.workers .Values.mpi.slotsPerWorker }} \
		--results-dir {{ .Values.storage.mountPath }}/{{ .Values.storage.resultsDir }} \
		--checkpoint-folder {{ .Values.storage.mountPath }}/{{ .Values.storage.checkpointDir }} \
		--model {{ .Values.benchmark.model }} \
		--num-checkpoints-write {{ .Values.benchmark.numCheckpointsWrite }} \
		--num-checkpoints-read {{ .Values.benchmark.numCheckpointsRead }} \
		--client-host-memory-in-gb {{ .Values.benchmark.clientHostMemoryGiB }} \
		--hosts <auto-derived>

CUSTOMIZATION NOTES
	* Increase parallelism: set mpi.workers and/or mpi.slotsPerWorker, CPU derives automatically.
	* Storage backend: change storage.pvcName to point at AMLFS / ANF / other shared storage.
	* Model flag is a sizing hint; adjust as needed for future logic.

UPGRADE EXAMPLE
	helm upgrade {{ .Release.Name }} ./mlperf-storage-checkpoint \
		--set mpi.workers=4 --set benchmark.numCheckpointsWrite=20

UNINSTALL
	helm uninstall {{ .Release.Name }}

ROOT / MPI WARNING
	Open MPI warns when running as root. Template passes --allow-run-as-root to enable execution
	inside the launcher container. For stricter security, run with a non-root user image and remove
	that flag.

HOST READINESS
	A 300s SSH readiness loop runs before the benchmark; if hosts are not reachable within the timeout
	the job exits with error.

