apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: {{ include "mlperf-training-dataset-generation.fullname" . }}
  labels:
    {{- include "mlperf-training-dataset-generation.labels" . | nindent 4 }}
    {{- if .Values.kueue.queueName }}
    kueue.x-k8s.io/queue-name: {{ .Values.kueue.queueName }}
    {{- end }}
spec:
  slotsPerWorker: {{ .Values.mpi.slotsPerWorker }}
  runPolicy:
    cleanPodPolicy: {{ .Values.mpi.cleanPodPolicy }}
    backoffLimit: {{ .Values.mpi.backoffLimit }}
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        metadata:
          labels:
            {{- include "mlperf-training-dataset-generation.selectorLabels" . | nindent 12 }}
            role: launcher
        spec:
          restartPolicy: OnFailure
          hostIPC: true
          containers:
          - name: mpi-launcher
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
              - /bin/bash
              - -c
              - |
                set -xuo pipefail
                HOSTS=$(sed 's/ slots=/:/' /etc/mpi/hostfile | paste -sd ',' -)
                START_TS=$(date +%s)
                TIMEOUT=300
                IFS=',' read -r -a HOST_ARR <<< "${HOSTS}"
                while true; do
                  ALL_READY=1
                  for entry in "${HOST_ARR[@]}"; do
                    host="${entry%%:*}"
                    if ! ssh -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=5 "${host}" true 2>/dev/null; then
                      ALL_READY=0
                    fi
                  done
                  if [ "${ALL_READY}" -eq 1 ]; then
                    break
                  fi
                  ELAPSED=$(( $(date +%s)-START_TS ))
                  if [ "${ELAPSED}" -ge "${TIMEOUT}" ]; then
                    echo "Error: Timeout waiting for all workers to be ready"
                    exit 1
                  fi
                  sleep 5
                done
                sleep 5
                $(which mlpstorage) training datagen \
                  --model {{ .Values.benchmark.model }} \
                  --num-processes "{{ mul .Values.mpi.workers .Values.mpi.slotsPerWorker }}"  \
                  --data-dir {{ .Values.storage.mountPath }}/{{ .Values.storage.dataDir }} \
                  --params dataset.num_files_train={{ .Values.benchmark.numFilesTrain }} \
                  --hosts ${HOSTS} {{- if .Values.benchmark.debug }} --debug {{- end }} {{- if .Values.benchmark.verbose }} --verbose {{- end }} {{- if .Values.benchmark.additionalParams }} {{ .Values.benchmark.additionalParams }} {{- end }}
            volumeMounts:
            - name: storage
              mountPath: {{ .Values.storage.mountPath }}
            resources:
              {{- toYaml .Values.launcher.resources | nindent 14 }}
          volumes:
          - name: storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
    Worker:
      replicas: {{ .Values.mpi.workers }}
      template:
        metadata:
          labels:
            {{- include "mlperf-training-dataset-generation.selectorLabels" . | nindent 12 }}
            role: worker
        spec:
          hostIPC: true
          containers:
          - name: mpi-worker
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
              - /usr/sbin/sshd
              - -De
            volumeMounts:
            - name: storage
              mountPath: {{ .Values.storage.mountPath }}
            resources:
              requests:
                cpu: "{{ .Values.mpi.slotsPerWorker }}"
                memory: {{ .Values.worker.resources.requests.memory | quote }}
              limits:
                cpu: "{{ .Values.mpi.slotsPerWorker }}"
                memory: {{ .Values.worker.resources.limits.memory | quote }}
          volumes:
          - name: storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
