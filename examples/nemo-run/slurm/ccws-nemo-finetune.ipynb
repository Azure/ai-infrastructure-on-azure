{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8938fe20",
   "metadata": {},
   "source": [
    "### Using nemo-run for Finetuning on CCWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52819f7f",
   "metadata": {},
   "source": [
    "**Step 1**: Load required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c01330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nemo_run as run\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from utils.run_tools import slurm_executor\n",
    "from nemo.collections import llm\n",
    "from typing import Optional\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419765a2",
   "metadata": {},
   "source": [
    "**Step 2**: Load required parameters from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "gpus = int( os.getenv(\"GPUS_PER_NODE\", \"8\") )\n",
    "nodes = int( os.getenv(\"NUM_NODES\", \"2\") )\n",
    "gpu_queue = os.getenv(\"GPU_PARTITION\")\n",
    "walltime = os.getenv(\"WALLTIME\", \"01:00:00\")\n",
    "container = os.getenv(\"CONTAINER_IMAGE\", \"nvcr.io#nvidia/nemo:dev\")\n",
    "nemo_home = os.getenv(\"NEMO_HOME\")\n",
    "checkpoint_dir = os.getenv(\"CHECKPOINT_DIR\")\n",
    "hf_token_path = os.getenv(\"HF_TOKEN_PATH\")\n",
    "hf_home = os.getenv(\"HF_HOME\")\n",
    "\n",
    "# verify nemo_home is set\n",
    "if nemo_home is None:\n",
    "    raise ValueError(\"NEMO_HOME environment variable is not set. \" \\\n",
    "    \"Please set it to the path of your NeMo installation.\")\n",
    "\n",
    "if checkpoint_dir is None:\n",
    "    raise ValueError(\"CHECKPOINT_PATH environment variable is not set. \" \\\n",
    "    \"Please set it to the path of your checkpoint directory.\")\n",
    "    \n",
    "# verify hf_token_path is set\n",
    "if hf_token_path is None:\n",
    "    raise ValueError(\"HF_TOKEN_PATH environment variable is not set. \" \\\n",
    "    \"Please set it to the path of your Hugging Face token.\")\n",
    "\n",
    "if hf_home is None: \n",
    "    raise ValueError(\"HF_HOME environment variable is not set. \" \\\n",
    "    \"Please set it to the path of your Hugging Face cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56590f",
   "metadata": {},
   "source": [
    "**Step 3**: Configuration - loading the llama3 model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3-8B\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "\n",
    "# define the slurm executor to perform the model import in a slurm job\n",
    "import_exec = slurm_executor(\n",
    "        account=\"\",\n",
    "        partition=gpu_queue,\n",
    "        nodes=1,\n",
    "        devices=1,\n",
    "        container_image=container,\n",
    "        gres=\"gpu:1\",  # Set the number of GPUs per node\n",
    "        custom_mounts=[nemo_home + \":\" + nemo_home,\n",
    "                       hf_token_path + \":\" + hf_token_path,\n",
    "                       hf_home + \":\" + hf_home ]\n",
    ")\n",
    "\n",
    "# Set this env vars for model download from huggingface\n",
    "import_exec.env_vars[\"NEMO_HOME\"] = nemo_home\n",
    "import_exec.env_vars[\"HF_HOME\"] = hf_home\n",
    "import_exec.env_vars[\"HF_TOKEN_PATH\"] = hf_token_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ee3b0",
   "metadata": {},
   "source": [
    "**Step 4**: Configuration - finetuning the model\n",
    "\n",
    "\n",
    "We will use the the default recipe included in Nemo 2.0 for finetuning the Llama 3 model. The default recipe uses the `SquadDataModule` for the `data` argument. You can replace the `SquadDataModule` with your custom dataset. We will not target the use of custom data sets for this exercise, but we included a template that highlights how to use custom data with the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe(nodes: int = 1, gpus_per_node: int = 1,  \n",
    "                                peft_scheme: Optional[str] = None, checkpoint_path: str = None, \n",
    "                                model_name: str = \"llama3_ccws\"):\n",
    "    recipe = llm.llama3_8b.finetune_recipe(\n",
    "        dir=checkpoint_path,  # Path to store checkpoints\n",
    "        name=model_name,\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "        peft_scheme=peft_scheme,\n",
    "    )\n",
    "\n",
    "    recipe.trainer.max_steps = 200\n",
    "    recipe.trainer.num_sanity_val_steps = 0\n",
    "\n",
    "    # Async checkpointing doesn't work with PEFT\n",
    "    recipe.trainer.strategy.ckpt_async_save = False\n",
    "\n",
    "    # Note, the default is 2\n",
    "    recipe.trainer.strategy.context_parallel_size = 1\n",
    "    recipe.trainer.val_check_interval = 200\n",
    "\n",
    "    # This is currently required for LoRA/PEFT\n",
    "    recipe.trainer.strategy.ddp = \"megatron\"\n",
    "\n",
    "    recipe.data.delete_raw = False\n",
    "    # # To override the data argument\n",
    "    # dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    #     gbs=gbs,\n",
    "    #     mbs=mbs,\n",
    "    #     seq_length=recipe.model.config.seq_length,\n",
    "    # )\n",
    "    # recipe.data = dataloader\n",
    "\n",
    "    return recipe\n",
    "\n",
    "model_name = \"llama3_ft_ccws\"\n",
    "# This will finetune the model using the number of nodes and GPUs specified in the .env file. \n",
    "# The .env file will also be used to determine where to store the checkpoints.\n",
    "finetune = configure_finetuning_recipe(gpus_per_node=gpus, nodes=nodes,\n",
    "                                       checkpoint_path=checkpoint_dir, model_name=model_name)\n",
    "\n",
    "# define the slurm executor to perform the finetuning as a slurm job\n",
    "finetune_exec = slurm_executor(\n",
    "    account=\"\",\n",
    "    partition=gpu_queue,\n",
    "    nodes=finetune.trainer.num_nodes,\n",
    "    devices=finetune.trainer.devices,\n",
    "    time=walltime,\n",
    "    container_image=container,\n",
    "    gres=\"gpu:\" + str(finetune.trainer.devices),\n",
    "    custom_mounts=[nemo_home + \":\" + nemo_home,\n",
    "                   hf_token_path + \":\" + hf_token_path,\n",
    "                   hf_home + \":\" + hf_home ]\n",
    ")\n",
    "\n",
    "finetune_exec.env_vars[\"NEMO_HOME\"] = nemo_home\n",
    "finetune_exec.env_vars[\"HF_HOME\"] = hf_home\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafe87c",
   "metadata": {},
   "source": [
    "**Step 6**: Submit jobs\n",
    "\n",
    "We will now use nemo-run experiments to finetune the model. The first job will load the model from huggingface, the second job will finetune the model as defined by the recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a569f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with run.Experiment(\"llama3-8b-nemoft-ccws\") as exp:\n",
    "    exp.add(import_ckpt, executor=import_exec, name=\"llama3-8b-ccws-import\")\n",
    "    exp.add(finetune, executor=finetune_exec, name=\"llama3-8b-ccws-finetune\")\n",
    "    exp.run(sequential=True, tail_logs=False, detach=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccb16d",
   "metadata": {},
   "source": [
    "**Step 7**: Monitor jobs\n",
    "\n",
    "Running exp.status() will let you know which tasks are being executed, and if they complete successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.status()\n",
    "\n",
    "# Uncomment the following lines to cancel the jobs if needed\n",
    "# rerun the previous cell to resubmit the jobs\n",
    "#exp.cancel(\"llama3-8b-ccws-finetune\")\n",
    "#exp.cancel(\"llama3-8b-ccws-import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec56cf6",
   "metadata": {},
   "source": [
    "**Optional**: Review Logs for model import\n",
    "\n",
    "As tasks are running, you can use exp.logs() to tail the logs in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.logs(\"llama3-8b-ccws-import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0247dcd",
   "metadata": {},
   "source": [
    "**Optional**: Review logs for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.logs(\"llama3-8b-ccws-finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca0315",
   "metadata": {},
   "source": [
    "**Step 8**: Verify Results\n",
    "\n",
    "The previous jobs, should produce a checkpoint. We will try to find the checkpoint's path so that we can inference against it. Do not proceed to the next steps if a path is not found. Verify that you have a checkpoint and ensure that `ckpt_path` is set to the path for the checkpoint.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will find the last checkpoint in the checkpoint directory and use it for inference\n",
    "search_path = ''.join([checkpoint_dir, \"/\", model_name, \"/\"])\n",
    "print(f\"Searching for checkpoints in {search_path}\")\n",
    "ckpt_path=str(next((d for d in Path(search_path).rglob(\"*\")\n",
    "                    if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "\n",
    "print(f\"Checkpoint path: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609586c",
   "metadata": {},
   "source": [
    "**Step 9**: Configure inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea685dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(devices: int, nodes: int) -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=devices,\n",
    "        num_nodes=nodes,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def configure_inference(prompts: list[str], sft_ckpt_path: str,\n",
    "                        output_path: str, nodes: int = 1, devices: int = 1,\n",
    "                        tokens: int = 1024, topp: float = 0.90, temp: float = 0.2):\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(nodes, devices),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=tokens,\n",
    "                                               top_p=topp, temperature=temp),\n",
    "        output_path=output_path\n",
    "    )\n",
    "\n",
    "# this will be the output path for the inference results\n",
    "output_path = ''.join([nemo_home, \"/llama3-8b-ft-ccws-prediction.jsonl\"])\n",
    "\n",
    "# this is the list of prompts to use for inference, feel free to edit them\n",
    "context = \"The Legend of Zelda is a video game series by Nintendo. Link is the main hero.\"\n",
    "question = \"Who is the main protagonist of the Legend of Zelda series?\"\n",
    "prompt1 = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "context = \"Queen Victoria ruled the United Kingdom from 1837 to 1901, marking the Victorian era.\"\n",
    "question = \"How long did Queen Victoria reign?\"\n",
    "prompt2  = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "context = \"The Eiffel Tower was built in Paris, France in 1889 and stands at 324 meters tall.\"\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "prompt3  = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "prompts = [prompt1, prompt2, prompt3]\n",
    "\n",
    "inference = configure_inference(prompts=prompts, sft_ckpt_path=ckpt_path, output_path=output_path)\n",
    "\n",
    "# define the slurm executor to perform the inferencing as a slurm job\n",
    "inference_exec = slurm_executor(\n",
    "        account=\"\",\n",
    "        partition=gpu_queue,\n",
    "        nodes=1,\n",
    "        devices=1,\n",
    "        container_image=container,\n",
    "        gres=\"gpu:1\",\n",
    "        custom_mounts=[nemo_home + \":\" + nemo_home,\n",
    "                       hf_token_path + \":\" + hf_token_path,\n",
    "                       hf_home + \":\" + hf_home]\n",
    "    )\n",
    "inference_exec.env_vars[\"NEMO_HOME\"] = nemo_home\n",
    "inference_exec.env_vars[\"HF_HOME\"] = hf_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab3cd0",
   "metadata": {},
   "source": [
    "**Step 10**: Inference the model\n",
    "\n",
    "If the job completes successfuly, you will find a file named: `llama3-8b-ft-ccws-prediction.jsonl` in your `NEMO_HOME` which includes the responses from the model to the provided prompts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4242652",
   "metadata": {},
   "outputs": [],
   "source": [
    "with run.Experiment(\"llama3-8b-nemoft-ccws\") as infexp:\n",
    "    infexp.add(inference, executor=inference_exec, name=\"llama3-8b-ft-ccws-inference\")\n",
    "    infexp.run(sequential=True, tail_logs=False, detach=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8df99",
   "metadata": {},
   "source": [
    "**Optional**: Check job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "infexp.status()\n",
    "# Uncomment the following lines to cancel the jobs if needed\n",
    "# rerun the previous cell to resubmit the jobs\n",
    "#infexp.cancel(\"llama3-8b-ft-ccws-inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3693867",
   "metadata": {},
   "source": [
    "**Optional**: Review logs for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "infexp.logs(\"llama3-8b-ft-ccws-inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00675b1e",
   "metadata": {},
   "source": [
    "Review the model responses from the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Output path: {output_path}\")\n",
    "\n",
    "print(\"Inference results:\")\n",
    "with open(output_path, 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c56078",
   "metadata": {},
   "source": [
    "If you are happy with your model, you can deploy it using any strategy you prefer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccws-nemo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
