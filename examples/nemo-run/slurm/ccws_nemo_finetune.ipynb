{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8938fe20",
   "metadata": {},
   "source": [
    "### Using nemo-run for Finetuning on CCWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52819f7f",
   "metadata": {},
   "source": [
    "**Step 1**: Load required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c01330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run a NeMo LLM inference job with specified parameters.\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import nemo_run as run\n",
    "from dotenv import load_dotenv\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from utils.run_tools import slurm_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419765a2",
   "metadata": {},
   "source": [
    "**Step 2**: Load required parameters from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GPUS = int(os.getenv(\"GPUS_PER_NODE\", \"8\"))\n",
    "NODES = int(os.getenv(\"NUM_NODES\", \"2\"))\n",
    "GPU_QUEUE = os.getenv(\"GPU_PARTITION\", \"gpu\")\n",
    "WALLTIME = os.getenv(\"WALLTIME\", \"01:00:00\")\n",
    "CONTAINER = os.getenv(\"CONTAINER_IMAGE\", \"nvcr.io#nvidia/nemo:dev\")\n",
    "NEMO_HOME = os.getenv(\"NEMO_HOME\")\n",
    "CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\")  # default to current directory\n",
    "HF_TOKEN_PATH = os.getenv(\"HF_TOKEN_PATH\")\n",
    "HF_HOME = os.getenv(\"HF_HOME\")\n",
    "\n",
    "# verify nemo_home is set\n",
    "if NEMO_HOME is None:\n",
    "    raise ValueError(\n",
    "        \"NEMO_HOME environment variable is not set. \"\n",
    "        \"Please set it to the path of your NeMo installation.\"\n",
    "    )\n",
    "\n",
    "if CHECKPOINT_DIR is None:\n",
    "    raise ValueError(\n",
    "        \"CHECKPOINT_PATH environment variable is not set. \"\n",
    "        \"Please set it to the path of your checkpoint directory.\"\n",
    "    )\n",
    "\n",
    "# verify hf_token_path is set\n",
    "if HF_TOKEN_PATH is None:\n",
    "    raise ValueError(\n",
    "        \"HF_TOKEN_PATH environment variable is not set. \"\n",
    "        \"Please set it to the path of your Hugging Face token.\"\n",
    "    )\n",
    "\n",
    "if HF_HOME is None:\n",
    "    raise ValueError(\n",
    "        \"HF_HOME environment variable is not set. \"\n",
    "        \"Please set it to the path of your Hugging Face cache.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56590f",
   "metadata": {},
   "source": [
    "**Step 3**: Configuration - loading the llama3 model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "def configure_checkpoint_conversion():\n",
    "    \"\"\"Configure the checkpoint conversion for NeMo LLM.\"\"\"\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3-8B\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "\n",
    "# define the slurm executor to perform the model import in a slurm job\n",
    "import_exec = slurm_executor(\n",
    "    account=\"\",\n",
    "    partition=GPU_QUEUE,\n",
    "    nodes=1,\n",
    "    devices=1,\n",
    "    container_image=CONTAINER,\n",
    "    gres=\"gpu:1\",  # Set the number of GPUs per node\n",
    "    custom_mounts=[\n",
    "        NEMO_HOME + \":\" + NEMO_HOME,\n",
    "        HF_TOKEN_PATH + \":\" + HF_TOKEN_PATH,\n",
    "        HF_HOME + \":\" + HF_HOME,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Set this env vars for model download from huggingface\n",
    "import_exec.env_vars[\"NEMO_HOME\"] = NEMO_HOME\n",
    "import_exec.env_vars[\"HF_HOME\"] = HF_HOME\n",
    "import_exec.env_vars[\"HF_TOKEN_PATH\"] = HF_TOKEN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ee3b0",
   "metadata": {},
   "source": [
    "**Step 4**: Configuration - finetuning the model\n",
    "\n",
    "\n",
    "We will use the the default recipe included in Nemo 2.0 for finetuning the Llama 3 model. The default recipe uses the `SquadDataModule` for the `data` argument. You can replace the `SquadDataModule` with your custom dataset. We will not target the use of custom data sets for this exercise, but we included a template that highlights how to use custom data with the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe(\n",
    "    checkpoint_path: str,\n",
    "    nodes: int = 1,\n",
    "    gpus_per_node: int = 1,\n",
    "    peft_scheme: Optional[str] = None,\n",
    "    model_name: str = \"llama3_ccws\",\n",
    "):\n",
    "    \"\"\"Configure the finetuning recipe for NeMo LLM.\"\"\"\n",
    "\n",
    "    recipe = llm.llama3_8b.finetune_recipe(\n",
    "        dir=checkpoint_path,  # Path to store checkpoints\n",
    "        name=model_name,\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "        peft_scheme=peft_scheme,\n",
    "    )\n",
    "\n",
    "    recipe.trainer.max_steps = 200\n",
    "    recipe.trainer.num_sanity_val_steps = 0\n",
    "\n",
    "    # Async checkpointing doesn't work with PEFT\n",
    "    recipe.trainer.strategy.ckpt_async_save = False\n",
    "\n",
    "    # Note, the default is 2\n",
    "    recipe.trainer.strategy.context_parallel_size = 1\n",
    "    recipe.trainer.val_check_interval = 200\n",
    "\n",
    "    # This is currently required for LoRA/PEFT\n",
    "    recipe.trainer.strategy.ddp = \"megatron\"\n",
    "\n",
    "    recipe.data.delete_raw = False\n",
    "    # # To override the data argument\n",
    "    # dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    #     gbs=gbs,\n",
    "    #     mbs=mbs,\n",
    "    #     seq_length=recipe.model.config.seq_length,\n",
    "    # )\n",
    "    # recipe.data = dataloader\n",
    "\n",
    "    return recipe\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama3_ft_ccws\"\n",
    "# This will finetune the model using the number of nodes and GPUs specified in the .env file.\n",
    "# The .env file will also be used to determine where to store the checkpoints.\n",
    "finetune = configure_finetuning_recipe(\n",
    "    checkpoint_path=CHECKPOINT_DIR,\n",
    "    gpus_per_node=GPUS,\n",
    "    nodes=NODES,\n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "# define the slurm executor to perform the finetuning as a slurm job\n",
    "finetune_exec = slurm_executor(\n",
    "    account=\"\",\n",
    "    partition=GPU_QUEUE,\n",
    "    nodes=finetune.trainer.num_nodes,\n",
    "    devices=finetune.trainer.devices,\n",
    "    time=WALLTIME,\n",
    "    container_image=CONTAINER,\n",
    "    gres=\"gpu:\" + str(finetune.trainer.devices),\n",
    "    custom_mounts=[\n",
    "        NEMO_HOME + \":\" + NEMO_HOME,\n",
    "        HF_TOKEN_PATH + \":\" + HF_TOKEN_PATH,\n",
    "        HF_HOME + \":\" + HF_HOME,\n",
    "    ],\n",
    ")\n",
    "\n",
    "finetune_exec.env_vars[\"NEMO_HOME\"] = NEMO_HOME\n",
    "finetune_exec.env_vars[\"HF_HOME\"] = HF_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafe87c",
   "metadata": {},
   "source": [
    "**Step 6**: Submit jobs\n",
    "\n",
    "We will now use nemo-run experiments to finetune the model. The first job will load the model from huggingface, the second job will finetune the model as defined by the recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a569f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with run.Experiment(\"llama3-8b-nemoft-ccws\") as exp:\n",
    "    exp.add(import_ckpt, executor=import_exec, name=\"llama3-8b-ccws-import\")\n",
    "    exp.add(finetune, executor=finetune_exec, name=\"llama3-8b-ccws-finetune\")\n",
    "    exp.run(sequential=True, tail_logs=False, detach=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccb16d",
   "metadata": {},
   "source": [
    "**Step 7**: Monitor jobs\n",
    "\n",
    "Running exp.status() will let you know which tasks are being executed, and if they complete successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.status()\n",
    "\n",
    "# Uncomment the following lines to cancel the jobs if needed\n",
    "# rerun the previous cell to resubmit the jobs\n",
    "# exp.cancel(\"llama3-8b-ccws-finetune\")\n",
    "# exp.cancel(\"llama3-8b-ccws-import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec56cf6",
   "metadata": {},
   "source": [
    "**Optional**: Review Logs for model import\n",
    "\n",
    "As tasks are running, you can use exp.logs() to tail the logs in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.logs(\"llama3-8b-ccws-import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0247dcd",
   "metadata": {},
   "source": [
    "**Optional**: Review logs for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.logs(\"llama3-8b-ccws-finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca0315",
   "metadata": {},
   "source": [
    "**Step 8**: Verify Results\n",
    "\n",
    "The previous jobs, should produce a checkpoint. We will try to find the checkpoint's path so that we can inference against it. Do not proceed to the next steps if a path is not found. Verify that you have a checkpoint and ensure that `ckpt_path` is set to the path for the checkpoint.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will find the last checkpoint in the checkpoint directory and use it for inference\n",
    "SEARCH_PATH = \"\".join([CHECKPOINT_DIR, \"/\", MODEL_NAME, \"/\"])\n",
    "print(f\"Searching for checkpoints in {SEARCH_PATH}\")\n",
    "\n",
    "CKPT_PATH = str(\n",
    "    next(\n",
    "        (\n",
    "            d\n",
    "            for d in Path(SEARCH_PATH).rglob(\"*\")\n",
    "            if d.is_dir() and d.name.endswith(\"-last\")\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Checkpoint path: {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609586c",
   "metadata": {},
   "source": [
    "**Step 9**: Configure inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea685dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(nodes: int, devices: int) -> run.Config[nl.Trainer]:\n",
    "    \"\"\"Configure the NeMo Trainer for inference.\"\"\"\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "    trainer_cfg = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=devices,\n",
    "        num_nodes=nodes,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer_cfg\n",
    "\n",
    "\n",
    "def configure_inference(\n",
    "    prompts: list[str],\n",
    "    sft_ckpt_path: str,\n",
    "    output_path: str,\n",
    "    nodes: int = 1,\n",
    "    devices: int = 1,\n",
    "    tokens: int = 1024,\n",
    "    topp: float = 0.90,\n",
    "    temp: float = 0.2,\n",
    "):\n",
    "    \"\"\"Configure the inference for NeMo LLM.\"\"\"\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(nodes, devices),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(\n",
    "            num_tokens_to_generate=tokens, top_p=topp, temperature=temp\n",
    "        ),\n",
    "        output_path=output_path,\n",
    "    )\n",
    "\n",
    "\n",
    "# this will be the output path for the inference results\n",
    "OUTPUT_PATH = \"\".join([NEMO_HOME, \"/llama3-8b-ft-ccws-prediction.jsonl\"])\n",
    "\n",
    "# this is the list of prompts to use for inference, feel free to edit them\n",
    "CONTEXT = (\n",
    "    \"The Legend of Zelda is a video game series by Nintendo. Link is the main hero.\"\n",
    ")\n",
    "QUESTION = \"Who is the main protagonist of the Legend of Zelda series?\"\n",
    "PROMPT1 = f\"Context: {CONTEXT}\\nQuestion: {QUESTION}\\nAnswer:\"\n",
    "\n",
    "CONTEXT = \"Queen Victoria ruled the United Kingdom from 1837 to 1901, marking the Victorian era.\"\n",
    "QUESTION = \"How long did Queen Victoria reign?\"\n",
    "PROMPT2 = f\"Context: {CONTEXT}\\nQuestion: {QUESTION}\\nAnswer:\"\n",
    "\n",
    "CONTEXT = (\n",
    "    \"The Eiffel Tower was built in Paris, France in 1889 and stands at 324 meters tall.\"\n",
    ")\n",
    "QUESTION = \"Where is the Eiffel Tower located?\"\n",
    "PROMPT3 = f\"Context: {CONTEXT}\\nQuestion: {QUESTION}\\nAnswer:\"\n",
    "\n",
    "PROMPTS = [PROMPT1, PROMPT2, PROMPT3]\n",
    "\n",
    "inference = configure_inference(\n",
    "    prompts=PROMPTS, sft_ckpt_path=CKPT_PATH, output_path=OUTPUT_PATH\n",
    ")\n",
    "\n",
    "# define the slurm executor\n",
    "# to perform the inferencing as a slurm job\n",
    "inference_exec = slurm_executor(\n",
    "    account=\"\",\n",
    "    partition=GPU_QUEUE,\n",
    "    nodes=1,\n",
    "    devices=1,\n",
    "    container_image=CONTAINER,\n",
    "    gres=\"gpu:1\",\n",
    "    custom_mounts=[\n",
    "        NEMO_HOME + \":\" + NEMO_HOME,\n",
    "        HF_TOKEN_PATH + \":\" + HF_TOKEN_PATH,\n",
    "        HF_HOME + \":\" + HF_HOME,\n",
    "    ],\n",
    ")\n",
    "inference_exec.env_vars[\"NEMO_HOME\"] = NEMO_HOME\n",
    "inference_exec.env_vars[\"HF_HOME\"] = HF_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab3cd0",
   "metadata": {},
   "source": [
    "**Step 10**: Inference the model\n",
    "\n",
    "If the job completes successfuly, you will find a file named: `llama3-8b-ft-ccws-prediction.jsonl` in your `NEMO_HOME` which includes the responses from the model to the provided prompts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4242652",
   "metadata": {},
   "outputs": [],
   "source": [
    "with run.Experiment(\"llama3-8b-nemoft-ccws\") as infexp:\n",
    "    infexp.add(\n",
    "        inference,\n",
    "        executor=inference_exec,\n",
    "        name=\"llama3-8b-ft-ccws-inference\",\n",
    "    )\n",
    "    infexp.run(sequential=True, tail_logs=False, detach=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8df99",
   "metadata": {},
   "source": [
    "**Optional**: Check job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "infexp.status()\n",
    "# Uncomment the following lines to cancel the jobs if needed\n",
    "# rerun the previous cell to resubmit the jobs\n",
    "# infexp.cancel(\"llama3-8b-ft-ccws-inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3693867",
   "metadata": {},
   "source": [
    "**Optional**: Review logs for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "infexp.logs(\"llama3-8b-ft-ccws-inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00675b1e",
   "metadata": {},
   "source": [
    "Review the model responses from the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "\n",
    "print(\"Inference results:\")\n",
    "with open(OUTPUT_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c56078",
   "metadata": {},
   "source": [
    "If you are happy with your model, you can deploy it using any strategy you prefer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccws-nemo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
