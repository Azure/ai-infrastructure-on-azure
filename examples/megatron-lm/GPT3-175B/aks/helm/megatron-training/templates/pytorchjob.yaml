apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: {{ include "megatron-training.fullname" . }}
  labels:
    {{- include "megatron-training.labels" . | nindent 4 }}
spec:
  {{- if .Values.pytorchJob.ttlSecondsAfterFinished }}
  ttlSecondsAfterFinished: {{ .Values.pytorchJob.ttlSecondsAfterFinished }}
  {{- end }}
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: {{ .Values.pytorchJob.restartPolicy }}
      template:
        metadata:
          labels:
            {{- include "megatron-training.labels" . | nindent 12 }}
            role: master
        spec:
          {{- with .Values.nodeSelector }}
          nodeSelector:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.affinity }}
          affinity:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.tolerations }}
          tolerations:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          hostNetwork: {{ .Values.networking.hostNetwork }}
          dnsPolicy: {{ .Values.networking.dnsPolicy }}
          containers:
          - name: pytorch
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
            - "bash"
            - "-c"
            args:
            - |
              set -xe
              
              # Run training script
              bash /scripts/train_megatron.sh
            env:
            - name: MASTER_ADDR
              value: "$(PYTORCHJOB_NAME)-master-0"
            - name: MASTER_PORT
              value: "23456"
            - name: STORAGE_MOUNT
              value: "{{ .Values.storage.mount }}"
            - name: DATASET_PATH
              value: "{{ .Values.storage.datasetPath }}"
            - name: LOGS_PATH
              value: "{{ .Values.storage.logsPath }}"
            - name: CHECKPOINT_PATH_DIR
              value: "{{ .Values.storage.checkpointPath }}"
            - name: CHUNKS
              value: "{{ .Values.training.chunks }}"
            - name: GLOBAL_BATCH_SIZE
              value: "{{ .Values.training.globalBatchSize }}"
            - name: NUMBER_OF_ITERATIONS
              value: "{{ .Values.training.iterations }}"
            - name: SAVE_INTERVAL
              value: "{{ .Values.training.saveInterval }}"
            - name: EVAL_INTERVAL
              value: "{{ .Values.training.evalInterval }}"
            - name: GPUS_PER_NODE
              value: "{{ .Values.training.gpusPerNode }}"
            - name: NODES
              value: "{{ .Values.training.nodes }}"
            - name: USE_SHARP
              value: "{{ .Values.training.useSharp }}"
            - name: LOGLEVEL
              value: "{{ .Values.env.logLevel }}"
            - name: TOPO_FILE
              value: "{{ .Values.env.topoFile }}"
            - name: NUM_LAYERS
              value: "{{ .Values.model.numLayers | default 96 }}"
            - name: HIDDEN_SIZE
              value: "{{ .Values.model.hiddenSize | default 12288 }}"
            - name: NUM_ATTENTION_HEADS
              value: "{{ .Values.model.numAttentionHeads | default 96 }}"
            - name: SEQ_LENGTH
              value: "{{ .Values.model.seqLength | default 2048 }}"
            - name: TENSOR_MODEL_PARALLEL_SIZE
              value: "{{ .Values.model.tensorModelParallelSize | default 8 }}"
            - name: PIPELINE_MODEL_PARALLEL_SIZE
              value: "{{ .Values.model.pipelineModelParallelSize | default 16 }}"
            resources:
              requests:
                nvidia.com/gpu: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
              limits:
                nvidia.com/gpu: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
            volumeMounts:
            - name: data-storage
              mountPath: {{ .Values.storage.mount }}
            - name: shm
              mountPath: /dev/shm
            - name: training-scripts
              mountPath: /scripts
              readOnly: true
            securityContext:
              privileged: true
              capabilities:
                add:
                - IPC_LOCK
          volumes:
          - name: data-storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: {{ .Values.resources.shmSize }}
          - name: training-scripts
            configMap:
              name: {{ include "megatron-training.fullname" . }}-scripts
              defaultMode: 0755
    {{- if gt (.Values.training.nodes | int) 1 }}
    Worker:
      replicas: {{ sub (.Values.training.nodes | int) 1 }}
      restartPolicy: {{ .Values.pytorchJob.restartPolicy }}
      template:
        metadata:
          labels:
            {{- include "megatron-training.labels" . | nindent 12 }}
            role: worker
        spec:
          {{- with .Values.nodeSelector }}
          nodeSelector:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.affinity }}
          affinity:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          {{- with .Values.tolerations }}
          tolerations:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          hostNetwork: {{ .Values.networking.hostNetwork }}
          dnsPolicy: {{ .Values.networking.dnsPolicy }}
          containers:
          - name: megatron
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            command:
            - "bash"
            - "-c"
            args:
            - |
              set -xe
              
              # Run training script
              bash /scripts/train_megatron.sh
            env:
            - name: MASTER_ADDR
              value: "$(PYTORCHJOB_NAME)-master-0"
            - name: MASTER_PORT
              value: "23456"
            - name: STORAGE_MOUNT
              value: "{{ .Values.storage.mount }}"
            - name: DATASET_PATH
              value: "{{ .Values.storage.datasetPath }}"
            - name: LOGS_PATH
              value: "{{ .Values.storage.logsPath }}"
            - name: CHECKPOINT_PATH_DIR
              value: "{{ .Values.storage.checkpointPath }}"
            - name: CHUNKS
              value: "{{ .Values.training.chunks }}"
            - name: GLOBAL_BATCH_SIZE
              value: "{{ .Values.training.globalBatchSize }}"
            - name: NUMBER_OF_ITERATIONS
              value: "{{ .Values.training.iterations }}"
            - name: SAVE_INTERVAL
              value: "{{ .Values.training.saveInterval }}"
            - name: EVAL_INTERVAL
              value: "{{ .Values.training.evalInterval }}"
            - name: GPUS_PER_NODE
              value: "{{ .Values.training.gpusPerNode }}"
            - name: NODES
              value: "{{ .Values.training.nodes }}"
            - name: USE_SHARP
              value: "{{ .Values.training.useSharp }}"
            - name: LOGLEVEL
              value: "{{ .Values.env.logLevel }}"
            - name: TOPO_FILE
              value: "{{ .Values.env.topoFile }}"
            - name: NUM_LAYERS
              value: "{{ .Values.model.numLayers | default 96 }}"
            - name: HIDDEN_SIZE
              value: "{{ .Values.model.hiddenSize | default 12288 }}"
            - name: NUM_ATTENTION_HEADS
              value: "{{ .Values.model.numAttentionHeads | default 96 }}"
            - name: SEQ_LENGTH
              value: "{{ .Values.model.seqLength | default 2048 }}"
            - name: TENSOR_MODEL_PARALLEL_SIZE
              value: "{{ .Values.model.tensorModelParallelSize | default 8 }}"
            - name: PIPELINE_MODEL_PARALLEL_SIZE
              value: "{{ .Values.model.pipelineModelParallelSize | default 16 }}"
            resources:
              requests:
                nvidia.com/gpu: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
              limits:
                nvidia.com/gpu: {{ .Values.training.gpusPerNode }}
                {{ .Values.resources.rdmaResource }}: {{ .Values.training.gpusPerNode }}
            volumeMounts:
            - name: data-storage
              mountPath: {{ .Values.storage.mount }}
            - name: shm
              mountPath: /dev/shm
            - name: training-scripts
              mountPath: /scripts
              readOnly: true
            securityContext:
              privileged: true
              capabilities:
                add:
                - IPC_LOCK
          volumes:
          - name: data-storage
            persistentVolumeClaim:
              claimName: {{ .Values.storage.pvcName }}
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: {{ .Values.resources.shmSize }}
          - name: training-scripts
            configMap:
              name: {{ include "megatron-training.fullname" . }}-scripts
              defaultMode: 0755
    {{- end }}
