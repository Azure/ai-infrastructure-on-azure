1. Get the status of your Megatron training job:
   kubectl get pytorchjob {{ include "megatron-training.fullname" . }}

2. Watch the training progress:
   kubectl logs -f {{ include "megatron-training.fullname" . }}-master-0

3. Check all worker logs:
   kubectl logs -l training.kubeflow.org/job-name={{ include "megatron-training.fullname" . }}

4. Check checkpoint status:
   kubectl run check-checkpoints --rm -i --tty --image={{ .Values.image.repository }}:{{ .Values.image.tag }} -- \
     bash -c "ls -la {{ .Values.storage.mount }}/{{ .Values.storage.checkpointPath }}/"

5. To scale the training, update the values and upgrade:
   helm upgrade {{ .Release.Name }} . --set training.nodes=4

Model Configuration:
- Nodes: {{ .Values.training.nodes }}
- GPUs per node: {{ .Values.training.gpusPerNode }}
- Total GPUs: {{ mul .Values.training.nodes .Values.training.gpusPerNode }}
- Global batch size: {{ .Values.training.globalBatchSize }}
- Training iterations: {{ .Values.training.iterations }}

{{- if .Values.training.useSharp }}
Note: SHARP acceleration is enabled for optimized collective communications.
{{- end }}
